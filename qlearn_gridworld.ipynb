{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolvendo o ambiente Cliff Walking com QLearning tabular\n",
    "\n",
    "O [Cliff Walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) é um dos mais simples ambientes de aprendizado por reforço (RL), sendo ideal para entender o funcionamento do QLearning tabular, um dos algoritmos mais fundamentais para iniciantes. A ideia do Cliff Walking é simples: o agente deve ser capaz de atravessar um tabuleiro do início ao fim tomando cuidado para não cair em um penhasco. Se o agente cair no penhasco, ele retorna para o início do tabuleiro e leva uma penalidade de recompensa.\n",
    "\n",
    "<img src=\"media/cliff_walking.gif\" width=\"200\">\n",
    "\n",
    "Abaixo seguem algumas informações importantes para a modelagem do ambiente como um Processo de Decisão de Markov (MDP):\n",
    "\n",
    "### Espaço de ações\n",
    "\n",
    "O espaço de ações é discreto e contém os inteiros do intervalo {0, 3}. Uma ação deve indicar a direção de um movimento:\n",
    "* 0: Cima\n",
    "* 1: Direita\n",
    "* 2: Baixo\n",
    "* 3: Esquerda\n",
    "\n",
    "### Espaço de estados\n",
    "\n",
    "O estado representa a posição do jogador no tabuleiro. Logo, o espaço de estados também é discreto e contém os inteiros do intervalo {0, 47}. O valor numérico da posição do agente no tabuleiro pode ser obtido como linha_atual * nlinhas + coluna, sendo que as linhas e colunas começam em 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliff_walking = gym.make(\n",
    "    \"CliffWalking-v0\",\n",
    "    # render_mode = 'human' # caso queira acompanhar o processo de treinamento visualmente, descomente essa linha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a tabela de QValores\n",
    "\n",
    "A tabela de Qvalores recebe como entrada um par (estado, ação) e deve retornar o QValor desse par. Um QValor pode ser interpretado como \"a recompensa acumulada total esperada por executar a ação A no estado S e depois seguir a mesma política até o final do episódio\". Logo, devemos ter uma linha da tabela para cada um dos 48 estados, sendo que cada linha deve ter uma coluna para cada uma das 4 ações. Além disso, os QValores devem possuir um valor inicial que, nesse caso, será zero.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"media/qtable.png\" alt=\"sample qtable\" width=\"400\">\n",
    "    <figcaption> Exemplo de tabela de Qvalores. Fonte: <a href=\"https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial\"> Datacamp </a> </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_q_table(n_states, n_actions):\n",
    "    return np.zeros((n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostrando ações com a política $\\epsilon$-greedy\n",
    "\n",
    "No final do treinamento, espera-se que a melhor ação para cada estado seja aquela cujo QValor é o maior. No entanto, para que o QLearn convirja adequadamente, é necessário que no início do treinamento o agente \"explore\" bem o ambiente. Isto é, que ele visite um grande número de estados mesmo que não sejam necessariamente ótimos. Uma técnica amplamente utilizada para essa finalidade é a política $\\epsilon$-greedy. Ela consiste em forçar o agente a escolher ações aleatoriamente com uma frequência que diminui conforme o treinamento avança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_table, state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, q_table.shape[1])\n",
    "    return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualização dos QValores utilizando a equação de Bellman\n",
    "\n",
    "A cada passo do treinamento, o agente executará uma ação e utilizará a informação retornada pelo ambiente para atualizar os seus QValores e, assim, aprender a tabela ótima. A atualização dos seus QValores é realizada através da equação de Bellman:\n",
    "\n",
    "$$Q_{t+1}(s_t, a_t) = (\\alpha - 1) Q_t (s_t, a_t) + \\alpha (R_{t+1} + \\gamma \\max_{a} Q_t (s_{t+1}, a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state, action, reward, next_state, terminated, alpha, gamma):\n",
    "    if terminated:\n",
    "        q_table[next_state] = np.zeros(q_table.shape[1]) # a recompensa esperada para o estado terminal é 0\n",
    "\n",
    "    q_table[state, action] = (alpha - 1) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de treinamento\n",
    "\n",
    "No loop de treinamento, juntaremos todas as funções desenvolvidas até o momento. A ideia principal é definir um número máximo de episódios (estágio inicial até o estágio final) para que o agente colete experiências do ambiente e otimize sua tabela de QValores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, \n",
    "          q_table,\n",
    "          n_episodes=3000, \n",
    "          epsilon=0.9, \n",
    "          epsilon_decay=0.999, \n",
    "          alpha=0.1, \n",
    "          gamma=0.99):\n",
    "     \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        if epsilon > 0.001:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        while not done and step < 1000: # 1000 steps max\n",
    "            action = get_action(q_table, state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            q_table = update_q_table(q_table, state, action, reward, next_state, terminated, alpha, gamma)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward} - epsilon: {epsilon}\")\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Total reward: -61469 - epsilon: 0.8991\n",
      "Episode 100 - Total reward: -10653 - epsilon: 0.8134986194699355\n",
      "Episode 200 - Total reward: -4311 - epsilon: 0.7360471625842407\n",
      "Episode 300 - Total reward: -2899 - epsilon: 0.6659696926115485\n",
      "Episode 400 - Total reward: -383 - epsilon: 0.6025641480906593\n",
      "Episode 500 - Total reward: -725 - epsilon: 0.545195309324691\n",
      "Episode 600 - Total reward: -565 - epsilon: 0.49328843452021\n",
      "Episode 700 - Total reward: -260 - epsilon: 0.44632350181590114\n",
      "Episode 800 - Total reward: -332 - epsilon: 0.4038299995153185\n",
      "Episode 900 - Total reward: -221 - epsilon: 0.3653822123303929\n",
      "Episode 1000 - Total reward: -20 - epsilon: 0.33059495641157327\n",
      "Episode 1100 - Total reward: -359 - epsilon: 0.29911972043659035\n",
      "Episode 1200 - Total reward: -124 - epsilon: 0.27064117409787486\n",
      "Episode 1300 - Total reward: -469 - epsilon: 0.24487400900939155\n",
      "Episode 1400 - Total reward: -13 - epsilon: 0.22156008038394912\n",
      "Episode 1500 - Total reward: -13 - epsilon: 0.2004658208452793\n",
      "Episode 1600 - Total reward: -120 - epsilon: 0.1813799004655124\n",
      "Episode 1700 - Total reward: -144 - epsilon: 0.16411110958546163\n",
      "Episode 1800 - Total reward: -113 - epsilon: 0.14848644320704313\n",
      "Episode 1900 - Total reward: -13 - epsilon: 0.13434936776657827\n",
      "Episode 2000 - Total reward: -17 - epsilon: 0.12155825292489168\n",
      "Episode 2100 - Total reward: -15 - epsilon: 0.109984952663304\n",
      "Episode 2200 - Total reward: -13 - epsilon: 0.09951352147043054\n",
      "Episode 2300 - Total reward: -87 - epsilon: 0.09003905275807701\n",
      "Episode 2400 - Total reward: -13 - epsilon: 0.08146662786906499\n",
      "Episode 2500 - Total reward: -13 - epsilon: 0.07371036514776484\n",
      "Episode 2600 - Total reward: -122 - epsilon: 0.06669255954658174\n",
      "Episode 2700 - Total reward: -119 - epsilon: 0.060342904148660566\n",
      "Episode 2800 - Total reward: -13 - epsilon: 0.054597785807743306\n",
      "Episode 2900 - Total reward: -13 - epsilon: 0.04939964784864245\n"
     ]
    }
   ],
   "source": [
    "q_table = new_q_table(n_states=48, n_actions=4)\n",
    "trained_q_table = train(cliff_walking, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
