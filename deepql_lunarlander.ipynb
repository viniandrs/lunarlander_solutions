{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "\n",
    "import utils.plots_cliffwalking as plots\n",
    "from env.cliff_walking import WindyCliffWalking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolvendo o ambiente Lunar Lander com Deep Q-Learning\n",
    "\n",
    "O [Cliff Walking](https://gymnasium.farama.org/environments/box2d/lunar_lander/) é consideravelmente mais complexo que o Cliff Walking, mas nos permitirá utilizar mais o potencial da rede neural do Deep Q-Learning. Nesse ambiente, o objetivo é pousar uma nave espacial na Lua, tendo como ações possíveis o controle de três motores e um vetor de estados com informações sobre a posição, velocidade, rotação e se as pernas da nave estão em contato com o chão. \n",
    "\n",
    "<img src=\"media/lunar_lander.gif\" width=\"200\">\n",
    "\n",
    "Abaixo seguem algumas informações importantes para a modelagem do ambiente como um Processo de Decisão de Markov (MDP):\n",
    "\n",
    "### Espaço de ações\n",
    "\n",
    "O espaço de ações é discreto e contém os inteiros do intervalo {0, 3}. As ações controlam a nave a partir de seus motores:\n",
    "* 0: Não fazer nada\n",
    "* 1: Ligar motor da esquerda\n",
    "* 2: Ligar motor principal\n",
    "* 3: Ligar motor da direita\n",
    "\n",
    "### Espaço de estados\n",
    "\n",
    "O vetor de estados possui oito elementos: posição (x, y) da nave, velocidade linear (x, y), ângulo, velocidade angular e dois valores booleanos que indicam se as pernas da nave estão ou não em contato com o chão. \n",
    "\n",
    "### Recompensas\n",
    "\n",
    "O agente recebe uma recompensa a cada passo. A recompensa de cada passo:\n",
    "\n",
    "- é aumentada/diminuída dependendo do quão perto/longe a nave está da plataforma de pouso.\n",
    "\n",
    "- é aumentada/diminuída dependendo do quão devagar/rápido a nave está se movendo.\n",
    "\n",
    "- é diminuída o quanto mais a nave estiver rotacionada.\n",
    "\n",
    "- é aumentada em 10 pontos para cada perna em contato com o chão.\n",
    "\n",
    "- é diminuída em 0.03 pontos a cada frame em que um motor lateral estiver ativo.\n",
    "\n",
    "- é diminuída em 0.3 pontos a cada frame que o motor principal está ativo.\n",
    "\n",
    "Além disso, o episódio recebe uma recompensa final de -100 ou +100 caso a nave colida ou pouse seguramente, respectivamente. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando o espaço de estados\n",
    "\n",
    "Por se tratarem de grandezas distintas, a faixa de valores para cada elemento do vetor de estados é diferente, o que pode levar a instabilidades no treinamento. Esse problema pode ser mitigado através da normalização do vetor de estados. No nosso caso, vamos fazer uma normalização minmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features range: [ 3.         3.        10.        10.         6.2831855 10.\n",
      "  1.         1.       ]\n"
     ]
    }
   ],
   "source": [
    "lunar_lander = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "low = lunar_lander.observation_space.low\n",
    "high = lunar_lander.observation_space.high\n",
    "\n",
    "print(f'features range: {high-low}')\n",
    "\n",
    "# defining low and high as constants tensors\n",
    "LOW = torch.tensor(low).float()\n",
    "HIGH = torch.tensor(high).float()\n",
    "\n",
    "def normalize_state(state) -> torch.Tensor:\n",
    "    return torch.div((state - LOW),  (HIGH - LOW)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo\n",
    "\n",
    "No Deep Q-Learning, os Q-valores de cada ação associados a um estado são calculados através de uma rede neural. Ou seja, a rede neural recebe como entrada um vetor de estados e deve retornar o vetor de Q-valores onde cada elemento representa o Q-valor da ação i. Um Q-valor pode ser interpretado como \"a recompensa acumulada total esperada por executar a ação A no estado S e depois seguir a mesma política até o final do episódio\".\n",
    "\n",
    "Por se tratar de um problema relativamente simples, o modelo para solucionar o CliffWalking pode ser uma rede MLP simples com uma única camada oculta. Além disso, note que a predição dos Q-valores é uma tarefa de regressão, portanto não há necessidade de softmax no final da rede. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes: List[int] = [64, 64]):\n",
    "        super().__init__()\n",
    "\n",
    "        n_actions = 4\n",
    "\n",
    "        layers = []\n",
    "        input_size = 8 # entrada: [x, y, v_x, v_y, theta, omega, left_leg, right_leg]\n",
    "        for n_neurons in layer_sizes:\n",
    "            layers.append(torch.nn.Linear(input_size, n_neurons))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_size = n_neurons\n",
    "        layers.append(torch.nn.Linear(input_size, n_actions))\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(layer.weight)\n",
    "                layer.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x: int):\n",
    "        x = normalize_state(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostrando ações com a política $\\epsilon$-greedy\n",
    "\n",
    "No final do treinamento, espera-se que a melhor ação para cada estado seja aquela cujo Q-Valor é o maior. No entanto, para que o Q-Learning convirja adequadamente, é necessário que no início do treinamento o agente \"explore\" bem o ambiente. Isto é, que o agente visite um grande número de estados mesmo que não sejam necessariamente ótimos. Uma técnica amplamente utilizada para essa finalidade é a política $\\epsilon$-greedy. Ela consiste em forçar o agente a escolher ações aleatoriamente com uma frequência que diminui conforme o treinamento avança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(model, state, epsilon, random=True, n_actions=4):\n",
    "    if torch.rand(1) < epsilon and random:\n",
    "        return torch.randint(n_actions, (1,)).item()\n",
    "    qvals = model(state)\n",
    "    return torch.argmax(qvals).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede neural \n",
    "\n",
    "A cada passo do treinamento, o agente executará uma ação e utilizará a informação retornada pelo ambiente para calcular uma loss e atualizar seus pesos de forma a minimizá-la. A loss que será utilizada é o erro quadrático médio entre o Q-valor escolhido e o maior Q-valor do próximo estado calculado utilizando a rede com os pesos anteriores à ultima atualização:\n",
    "\n",
    "$$L_i(\\theta_i)=\\mathbb{E}[(y_i - Q(s,a;\\theta_i))^2]$$\n",
    "$$y_i=\\mathbb{E}[R(s')+\\gamma\\max_A Q(s',A;\\theta_{i-1})]$$\n",
    "\n",
    "Note que, portanto, serão necessárias duas redes neurais com a mesma arquitetura, mas uma terá os pesos defasados em uma iteração com relação à outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_net(model: torch.nn.Module, \n",
    "                 model_target: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer, \n",
    "                 state, action, reward, next_state, terminated, gamma):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(state)[action]\n",
    "\n",
    "    # se o estado é terminal, o valor é a recompensa\n",
    "    with torch.no_grad():\n",
    "        target = reward + gamma * torch.max(model_target(next_state)) if not terminated else torch.tensor(reward, dtype=torch.float32) \n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(prediction, target)\n",
    "    loss.backward()\n",
    "\n",
    "    # atualizando os pesos da rede defasada\n",
    "    model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "    # atualizando os pesos da rede\n",
    "    optimizer.step()\n",
    "\n",
    "    return model, model_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treinamento\n",
    "\n",
    "No loop de treinamento, juntaremos todas as funções desenvolvidas até o momento. A ideia principal é definir um número máximo de episódios (estágio inicial até o estágio final) para que o agente colete experiências do ambiente e otimize sua tabela de QValores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, \n",
    "          model,\n",
    "          n_episodes=5000, # numero maximo de episodios\n",
    "          epsilon=0.9, # probabilidade inicial de escolher uma ação aleatória\n",
    "          epsilon_decay=0.999, # fator de decaimento da probabilidade de escolher uma ação aleatória\n",
    "          learning_rate=0.0001, # taxa de aprendizado\n",
    "          gamma=0.99, # fator de desconto\n",
    "          verbose=False):\n",
    "    \n",
    "    model_target = deepcopy(model)\n",
    "\n",
    "    model.initialize()\n",
    "    model_target.initialize()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_rewards = []\n",
    "    avg_rewards = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        if epsilon > 0.001:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        while not done and step < 500: # 1000 steps max\n",
    "            action = get_action(model, state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            model, model_target = update_q_net(model, model_target, optimizer, state, action, reward, next_state, terminated, gamma)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        avg_reward = sum(total_rewards[-50:]) / len(total_rewards[-50:])\n",
    "        if episode % 50:\n",
    "            avg_rewards.append(avg_reward)\n",
    "        if episode % 100 == 0 and verbose:\n",
    "            clear_output(wait=True)\n",
    "            debug_model.load_state_dict(model.state_dict())\n",
    "            print(f\"Episode {episode} - Average total reward: {avg_reward} - epsilon: {epsilon}\")\n",
    "            plt.plot(avg_rewards)\n",
    "            plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando\n",
    "\n",
    "Está tudo configurado, portanto agora podemos rodar o algoritmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "div(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m q_net \u001b[38;5;241m=\u001b[39m Qnet(layer_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m])\n\u001b[1;32m      3\u001b[0m debug_model \u001b[38;5;241m=\u001b[39m deepcopy(q_net)\n\u001b[0;32m----> 4\u001b[0m trained_q_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlunar_lander\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, model, n_episodes, epsilon, epsilon_decay, learning_rate, gamma, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m     epsilon \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon_decay\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m: \u001b[38;5;66;03m# 1000 steps max\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     31\u001b[0m     model, model_target \u001b[38;5;241m=\u001b[39m update_q_net(model, model_target, optimizer, state, action, reward, next_state, terminated, gamma)\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mget_action\u001b[0;34m(model, state, epsilon, random, n_actions)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m epsilon \u001b[38;5;129;01mand\u001b[39;00m random:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandint(n_actions, (\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m----> 4\u001b[0m qvals \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(qvals)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/lunar_lander/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lunar_lander/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mQnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mnormalize_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_state\u001b[39m(state) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mTypeError\u001b[0m: div(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "lunar_lander = gym.make('LunarLander-v2')\n",
    "q_net = Qnet(layer_sizes=[128, 128])\n",
    "debug_model = deepcopy(q_net)\n",
    "trained_q_net = train(lunar_lander, q_net, n_episodes=5000, learning_rate=1e-5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o agente\n",
    "\n",
    "A função abaixo rodará um episódio com o agente já treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env: gym.Env, \n",
    "          q_net,\n",
    "          n_episodes=1,\n",
    "          verbose=False\n",
    "          ):\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(q_net, state, 0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward}\")\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return torch.mean(total_reward)\n",
    "\n",
    "#test(gym.make('CliffWalking-v0', render_mode=\"human\"), trained_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de resultados\n",
    "\n",
    "### Visualizando a política\n",
    "\n",
    "A célula abaixo permitirá observar a ação mais provável de ser tomada em cada uma das posições do tabuleiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(plots)\n",
    "plots.plot_arrows_from_qnet(debug_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0: [-99.32302 -99.32125 -99.32189 -99.39064]\n",
      "state 1: [-99.37512  -99.38653  -99.397316 -99.32632 ]\n",
      "state 2: [-99.42722 -99.45182 -99.47275 -99.262  ]\n",
      "state 3: [-99.479324 -99.517105 -99.54818  -99.19767 ]\n",
      "state 4: [-99.531425 -99.58239  -99.62361  -99.13336 ]\n",
      "state 5: [-99.58353  -99.64768  -99.69904  -99.069046]\n",
      "state 6: [-99.63563  -99.712975 -99.77447  -99.00472 ]\n",
      "state 7: [-99.68773 -99.77827 -99.8499  -98.9404 ]\n",
      "state 8: [-99.73983  -99.843544 -99.92533  -98.87608 ]\n",
      "state 9: [ -99.79193   -99.90884  -100.00076   -98.811775]\n",
      "state 10: [ -99.844025  -99.97412  -100.07619   -98.747444]\n",
      "state 11: [ -99.896126 -100.03941  -100.15163   -98.68313 ]\n",
      "state 12: [ -99.34171  -101.59196   -99.90222   -99.441574]\n",
      "state 13: [ -99.39381 -101.65725  -99.97765  -99.37725]\n",
      "state 14: [ -99.4459   -101.722534 -100.05308   -99.312935]\n",
      "state 15: [ -99.498   -101.78782 -100.12851  -99.24861]\n",
      "state 16: [ -99.55011  -101.85311  -100.20393   -99.184296]\n",
      "state 17: [ -99.60221  -101.918396 -100.27936   -99.11998 ]\n",
      "state 18: [ -99.65431  -101.983696 -100.3548    -99.05566 ]\n",
      "state 19: [ -99.70641 -102.04897 -100.43023  -98.99134]\n",
      "state 20: [ -99.75851 -102.11426 -100.50566  -98.92702]\n",
      "state 21: [ -99.81061  -102.17956  -100.581085  -98.8627  ]\n",
      "state 22: [ -99.86272  -102.244835 -100.65652   -98.798386]\n",
      "state 23: [ -99.91482 -102.31013 -100.73194  -98.73407]\n",
      "state 24: [ -99.3604   -103.86267  -100.482544  -99.492516]\n",
      "state 25: [ -99.412506 -103.92795  -100.55798   -99.42819 ]\n",
      "state 26: [ -99.46461 -103.99325 -100.63341  -99.36387]\n",
      "state 27: [ -99.5167  -104.05853 -100.70883  -99.29955]\n",
      "state 28: [ -99.5688  -104.12383 -100.78426  -99.23523]\n",
      "state 29: [ -99.620895 -104.18911  -100.85969   -99.17091 ]\n",
      "state 30: [ -99.673    -104.254395 -100.93512   -99.10659 ]\n",
      "state 31: [ -99.72509  -104.319695 -101.01056   -99.042274]\n",
      "state 32: [ -99.77719  -104.384964 -101.085976  -98.97795 ]\n",
      "state 33: [ -99.82929  -104.450264 -101.16141   -98.91363 ]\n",
      "state 34: [ -99.93114 -104.5434  -101.28825  -98.90704]\n",
      "state 35: [-100.20399 -104.73222 -101.59181  -99.09897]\n",
      "state 36: [ -99.37909  -106.13338  -101.062874  -99.54344 ]\n",
      "state 37: [ -99.43118  -106.19866  -101.1383    -99.479126]\n",
      "state 38: [ -99.483284 -106.26395  -101.21373   -99.4148  ]\n",
      "state 39: [ -99.535416 -106.329216 -101.289116  -99.35054 ]\n",
      "state 40: [ -99.58781 -106.39426 -101.36395  -99.28673]\n",
      "state 41: [ -99.64019 -106.4593  -101.43882  -99.22292]\n",
      "state 42: [ -99.69258  -106.52434  -101.513664  -99.1591  ]\n",
      "state 43: [ -99.85878  -106.65307  -101.70614   -99.227394]\n",
      "state 44: [-100.13191 -106.84167 -102.00914  -99.4198 ]\n",
      "state 45: [-100.405045 -107.03025  -102.312126  -99.61221 ]\n",
      "state 46: [-100.67817 -107.21883 -102.61512  -99.80463]\n",
      "state 47: [-100.951294 -107.40743  -102.91811   -99.99704 ]\n"
     ]
    }
   ],
   "source": [
    "for state in range(48):\n",
    "    print(f'state {state}: {debug_model(state).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando a influência da taxa de aprendizado\n",
    "\n",
    "A célula abaixo criará um gráfico com a recompensa total média de um conjunto de agentes treinados com uma diferente taxa de aprendizado. Como você explicaria os resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[1;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mplot_avg_return_x_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m, in \u001b[0;36mplot_avg_return_x_alpha\u001b[0;34m(n_samples, n_alphas)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_avg_return_x_alpha\u001b[39m(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m ):\n\u001b[1;32m      4\u001b[0m     avg_returns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m     alphas\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_alphas)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[1;32m      8\u001b[0m         q_table \u001b[38;5;241m=\u001b[39m new_q_table(n_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, n_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# retorno médio esperado em função de alpha\n",
    "\n",
    "def plot_avg_return_x_alpha(n_samples=100, n_alphas=10 ):\n",
    "    avg_returns = []\n",
    "    alphas=np.linspace(0.1, 1, n_alphas)\n",
    " \n",
    "    for alpha in alphas:\n",
    "        q_table = new_q_table(n_states=48, n_actions=4)\n",
    "        trained_q_table = train(cliff_walking, q_table, alpha=alpha)\n",
    "        avg_returns.append(test(cliff_walking, trained_q_table, n_samples))\n",
    "\n",
    "    plt.plot(alphas, avg_returns)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_avg_return_x_alpha(n_samples=100, n_alphas=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício - ambientes não-determinísticos\n",
    "\n",
    "Agora que já vimos que o agente aprendeu um caminho para o objetivo que minimiza a distância caminhada e não cai no penhasco, está na hora de deixar as coisas um pouco mais difíceis. Utilizaremos agora uma versão modificada do Cliff Walking que adiciona vento aleatório na direção do penhasco. Ou seja, existe uma probabilidade do agente ser deslocado uma casa para baixo de maneira involuntária.\n",
    "\n",
    "Essa alteração será interessante para observarmos a infuência da taxa de aprendizado no treinamento de um modelo robusto a ambientes não-determinísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windy_cliff_walking = WindyCliffWalking(wind=0.5)\n",
    "\n",
    "windy_q_table = new_q_table(n_states=48, n_actions=4)\n",
    "trained_windy_q_table = train(cliff_walking, windy_q_table, alpha=0.75, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windy_cliff_walking.render_mode = 'human'\n",
    "test(windy_cliff_walking, trained_windy_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_arrows(trained_q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
