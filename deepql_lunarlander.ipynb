{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viniciusandreossi/anaconda3/envs/cleanrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7efd24e7ad70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.plots_cliffwalking as plots\n",
    "from env.cliff_walking import WindyCliffWalking\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolvendo o ambiente Cliff Walking com Deep Q-Learning\n",
    "\n",
    "O [Cliff Walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) será utilizado novamente para entendermos o funcionamento do Deep Q-Learning, o algoritmo mais simples de Deep Reinforcement Learning. Recapitulando o objetivo do Cliff Walking, o agente deve ser capaz de atravessar um tabuleiro do início ao fim tomando cuidado para não cair em um penhasco. Se o agente cair no penhasco, ele retorna para o início do tabuleiro e leva uma penalidade de recompensa.\n",
    "\n",
    "<img src=\"media/cliff_walking.gif\" width=\"200\">\n",
    "\n",
    "Abaixo seguem algumas informações importantes para a modelagem do ambiente como um Processo de Decisão de Markov (MDP):\n",
    "\n",
    "### Espaço de ações\n",
    "\n",
    "O espaço de ações é discreto e contém os inteiros do intervalo {0, 3}. Uma ação deve indicar a direção de um movimento:\n",
    "* 0: Cima\n",
    "* 1: Direita\n",
    "* 2: Baixo\n",
    "* 3: Esquerda\n",
    "\n",
    "### Espaço de estados\n",
    "\n",
    "O estado representa a posição do jogador no tabuleiro. Logo, o espaço de estados também é discreto e contém os inteiros do intervalo {0, 47}. O valor numérico da posição do agente no tabuleiro pode ser obtido como linha_atual * nlinhas + coluna, sendo que as linhas e colunas começam em 0.\n",
    "\n",
    "### Recompensas\n",
    "\n",
    "A cada movimento do agente uma penalidade de -1 é aplicada, a menos que o jogador caia do penhasco, o que resulta em uma penalidade -100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo\n",
    "\n",
    "No Deep Q-Learning, os Q-valores de cada ação associados a um estado são calculados através de uma rede neural. Ou seja, a rede neural recebe como entrada um vetor de estados e deve retornar o vetor de Q-valores onde cada elemento representa o Q-valor de uma ação. Um Q-valor pode ser interpretado como \"a recompensa acumulada total esperada por executar a ação A no estado S e depois seguir a mesma política até o final do episódio\".\n",
    "\n",
    "Por se tratar de um problema relativamente simples, o modelo para solucionar o CliffWalking pode ser uma rede MLP. Além disso, note que a predição dos Q-valores é uma tarefa de regressão, portanto não é utilizada uma função de ativação softmax no final da rede. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão de inteiro para (x,y)\n",
    "\n",
    "A rede neural fica mais simples se receber como entrada um par (x,y) em vez de um inteiro. Se fossemos alimentá-la com um inteiro, teríamos que converter esse inteiro para um one-hot. Como são 48 estados, precisaríamos de um vetor de 48 elementos, sendo 47 elementos zerados! Ao passarmos um par (x,y), o vetor fica reduzido para apenas dois elementos. A função abaixo codifica o estado para a posição do agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, layer_sizes: List[int] = [64, 64], n_actions=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # construindo a rede neural\n",
    "        layers = []\n",
    "        input_size = 8\n",
    "        for n_neurons in layer_sizes:\n",
    "            layers.append(nn.Linear(input_size, n_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = n_neurons\n",
    "        layers.append(nn.Linear(input_size, n_actions))\n",
    "        self.nn = nn.Sequential(*layers)\n",
    "\n",
    "    def _encode_state(self, state) -> torch.Tensor:\n",
    "        return torch.tensor(state, dtype=torch.float32).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._encode_state(x)\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Replay buffers são utilizados (...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1024, backup_fraction=0.1):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\" Save a transition into the buffer.\"\"\"\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostrando ações com a política $\\epsilon$-greedy\n",
    "\n",
    "No final do treinamento, espera-se que a melhor ação para cada estado seja aquela cujo Q-Valor é o maior. No entanto, para que o Q-Learning convirja adequadamente, é necessário que no início do treinamento o agente \"explore\" bem o ambiente. Isto é, que o agente visite um grande número de estados mesmo que não sejam necessariamente ótimos. Uma técnica amplamente utilizada para essa finalidade é a política $\\epsilon$-greedy. Ela consiste em forçar o agente a escolher ações aleatoriamente com uma frequência que diminui conforme o treinamento avança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_action(model, state, epsilon, n_actions=4):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        return torch.randint(n_actions, (1,)).item()\n",
    "    qvals = model(state)\n",
    "    return torch.argmax(qvals).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede neural \n",
    "\n",
    "A cada passo do treinamento, o agente executará uma ação e utilizará a informação retornada pelo ambiente para calcular uma loss e atualizar seus pesos de forma a minimizá-la. A loss que será utilizada é o erro quadrático médio entre o Q-valor escolhido e o maior Q-valor do próximo estado calculado utilizando a rede com os pesos anteriores à ultima atualização:\n",
    "\n",
    "$$L_i(\\theta_i)=\\mathbb{E}[(y_i - Q(s,a;\\theta_i))^2]$$\n",
    "$$y_i=\\mathbb{E}[R(s')+\\gamma\\max_A Q(s',A;\\theta_{i-1})]$$\n",
    "\n",
    "Note que, portanto, serão necessárias duas redes neurais com a mesma arquitetura, mas uma terá os pesos deefasados em uma iteração com relação à outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_net(model: Qnet, \n",
    "                 model_target: Qnet,\n",
    "                 optimizer: torch.optim.Optimizer, \n",
    "                 batch_of_transitions: List[Transition],\n",
    "                 gamma):\n",
    "    \n",
    "    # Convert a list of Transitions into a Transition of lists\n",
    "    # Check: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Transition(*zip(*batch_of_transitions))\n",
    "    \n",
    "    # transformando o batch de estados em um tensor\n",
    "    states = torch.tensor(batch.state, dtype=torch.float32)\n",
    "    actions = torch.tensor(batch.action, dtype=torch.int64)\n",
    "    rewards = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "    next_states = torch.tensor(batch.next_state, dtype=torch.float32)\n",
    "    terminated = torch.tensor(batch.terminated, dtype=torch.float32)\n",
    "    \n",
    "    predictions = model(states).gather(1, actions.unsqueeze(1)).squeeze() # seleciona o qval da acao tomada\n",
    "\n",
    "    # se o estado é terminal, o valor é a recompensa\n",
    "    with torch.no_grad():\n",
    "        targets = rewards + gamma * model_target(next_states).max(-1).values * (1 - terminated.int()) # else rewards + gamma * max_a Q(s', a)\n",
    "    \n",
    "    loss = F.mse_loss(predictions, targets)\n",
    "\n",
    "    # atualizando os pesos da rede\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treinamento\n",
    "\n",
    "No loop de treinamento, juntaremos todas as funções desenvolvidas até o momento. A ideia principal é definir um número máximo de episódios (estágio inicial até o estágio final) para que o agente colete experiências do ambiente e otimize sua tabela de QValores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics: Dict, show_result=False):\n",
    "    rewards = torch.tensor(metrics['episode_rewards'])\n",
    "    plt.figure(1)\n",
    "    plt.clf\n",
    "    plt.title('Total reward of each episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "    plt.plot(rewards)\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if metrics['avg_reward'] is not None: \n",
    "        x = range(49, 49 + len(metrics['avg_reward']))\n",
    "        plt.plot(x, metrics['avg_reward'].numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    if not show_result:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        env: gym.Env, \n",
    "        model: Qnet,\n",
    "        total_steps=250_000,\n",
    "        replay_buffer_size=10_000, # tamanho do replay buffer\n",
    "        batch_size=64, # tamanho do batch\n",
    "        gamma=0.99,\n",
    "        learning_rate=1e-4,\n",
    "        learning_freq=1,\n",
    "        target_update_freq=10,\n",
    "        tau=1.0,\n",
    "        epsilon_0 = 1, # probabilidade inicial de escolher uma ação aleatória\n",
    "        epsilon_f=0.05, # probabilidade final de escolher uma ação aleatória (após o decaimento)\n",
    "        epsilon_decay = 10_000, # step in which epsilon will be approximately epsilon_f + 0.36 * (epsilon_0 - epsilon_f)\n",
    "        verbose=False):\n",
    "    \n",
    "    model_target = deepcopy(model)\n",
    "\n",
    "    def eps_scheduler(step):\n",
    "        return epsilon_f + (epsilon_0 - epsilon_f) * math.exp(-1. * step / epsilon_decay)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    metrics = {\n",
    "        'episode_rewards': [],\n",
    "        'avg_reward': None,\n",
    "    }\n",
    "    episode_step = 0\n",
    "    truncated = False\n",
    "\n",
    "    for global_step in range(total_steps):\n",
    "        epsilon = eps_scheduler(global_step)\n",
    "\n",
    "        # observe\n",
    "        action = get_action(model, state, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        episode_step += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # update\n",
    "        if global_step > batch_size and global_step % learning_freq == 0:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            model = update_q_net(model, model_target, optimizer, batch, gamma)\n",
    "\n",
    "        # update target network\n",
    "        if global_step % target_update_freq == 0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "        # evaluating\n",
    "        if done:\n",
    "            metrics['episode_rewards'].append(total_reward)\n",
    "            if len(metrics['episode_rewards']) > 50:\n",
    "                metrics['avg_reward'] = torch.tensor(metrics['episode_rewards']).float().unfold(0, 50, 1).mean(1)\n",
    "                print(f'avg reward: {metrics[\"avg_reward\"][-1]}')\n",
    "            print(f'current step: {global_step}, epsilon: {epsilon:.2f}')\n",
    "            # if global_step % 10_000 == 0:\n",
    "            #     evaluate(metrics)\n",
    "            evaluate(metrics)\n",
    "            total_reward = 0\n",
    "            state, _ = env.reset()\n",
    "            episode_step = 0\n",
    "            truncated = False\n",
    "\n",
    "    evaluate(metrics, show_result=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando\n",
    "\n",
    "Está tudo configurado, portanto agora podemos rodar o algoritmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "cliff_walking = gym.make('LunarLander-v2')\n",
    "q_net = Qnet()\n",
    "debug_model = deepcopy(q_net)\n",
    "trained_q_net = train(cliff_walking, q_net, epsilon_decay=50_000, learning_rate=1e-3, replay_buffer_size=2 ** 20, target_update_freq=100, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o agente\n",
    "\n",
    "A função abaixo rodará um episódio com o agente já treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(181.7814)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(env: gym.Env, \n",
    "          q_net,\n",
    "          n_episodes=1,\n",
    "          verbose=False\n",
    "          ):\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(q_net, state, 0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward}\")\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    total_reward = torch.tensor(total_rewards, dtype=torch.float32)\n",
    "    return torch.mean(total_reward)\n",
    "\n",
    "test(gym.make('LunarLander-v2', render_mode=\"human\"), trained_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de resultados\n",
    "\n",
    "### Visualizando a política\n",
    "\n",
    "A célula abaixo permitirá observar a ação mais provável de ser tomada em cada uma das posições do tabuleiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24739/1860028285.py:20: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  y = (state // 12) / 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m reload(plots)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_arrows_from_qnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rl_practice_eldorado/utils/plots_cliffwalking.py:64\u001b[0m, in \u001b[0;36mplot_arrows_from_qnet\u001b[0;34m(q_net)\u001b[0m\n\u001b[1;32m     61\u001b[0m normalized_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(qvals\u001b[38;5;241m/\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# cima\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_q_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# direita\u001b[39;00m\n\u001b[1;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39marrow(x, y, normalized_q_values[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m, head_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, head_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, fc\u001b[38;5;241m=\u001b[39ma, ec\u001b[38;5;241m=\u001b[39ma)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/pyplot.py:2347\u001b[0m, in \u001b[0;36marrow\u001b[0;34m(x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39marrow)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marrow\u001b[39m(x, y, dx, dy, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4961\u001b[0m, in \u001b[0;36mAxes.arrow\u001b[0;34m(self, x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   4958\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_xunits(dx)\n\u001b[1;32m   4959\u001b[0m dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(dy)\n\u001b[0;32m-> 4961\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mmpatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFancyArrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_patch(a)\n\u001b[1;32m   4963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view()\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/patches.py:1350\u001b[0m, in \u001b[0;36mFancyArrow.__init__\u001b[0;34m(self, x, y, dx, dy, width, length_includes_head, head_width, head_length, shape, overhang, head_starts_at_zero, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overhang \u001b[38;5;241m=\u001b[39m overhang\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head_starts_at_zero \u001b[38;5;241m=\u001b[39m head_starts_at_zero\n\u001b[0;32m-> 1350\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_verts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverts, closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/patches.py:1409\u001b[0m, in \u001b[0;36mFancyArrow._make_verts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m     length \u001b[38;5;241m=\u001b[39m distance \u001b[38;5;241m+\u001b[39m head_length\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m length:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# display nothing if empty\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;66;03m# start by drawing horizontal arrow, point at (0, 0)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(plots)\n",
    "plots.plot_arrows_from_qnet(debug_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
