{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f58efd7c550>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.plots_cliffwalking as plots\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo\n",
    "\n",
    "No Deep Q-Learning, os Q-valores de cada ação associados a um estado são calculados através de uma rede neural. Ou seja, a rede neural recebe como entrada um vetor de estados e deve retornar o vetor de Q-valores onde cada elemento representa o Q-valor de uma ação. Um Q-valor pode ser interpretado como \"a recompensa acumulada total esperada por executar a ação A no estado S e depois seguir a mesma política até o final do episódio\".\n",
    "\n",
    "Por se tratar de um problema relativamente simples, o modelo para solucionar o CliffWalking pode ser uma rede MLP. Além disso, note que a predição dos Q-valores é uma tarefa de regressão, portanto não é utilizada uma função de ativação softmax no final da rede. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes: List[int] = [32]):\n",
    "        super().__init__()\n",
    "\n",
    "        # construindo a rede neural\n",
    "        layers = []\n",
    "        input_size = 4 # entrada: posicao (x, y)\n",
    "        for n_neurons in layer_sizes:\n",
    "            layers.append(torch.nn.Linear(input_size, n_neurons))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            input_size = n_neurons\n",
    "        layers.append(torch.nn.Linear(input_size, 2))\n",
    "        self.nn = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._encode(x)\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Replay buffers são utilizados (...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostrando ações com a política $\\epsilon$-greedy\n",
    "\n",
    "No final do treinamento, espera-se que a melhor ação para cada estado seja aquela cujo Q-Valor é o maior. No entanto, para que o Q-Learning convirja adequadamente, é necessário que no início do treinamento o agente \"explore\" bem o ambiente. Isto é, que o agente visite um grande número de estados mesmo que não sejam necessariamente ótimos. Uma técnica amplamente utilizada para essa finalidade é a política $\\epsilon$-greedy. Ela consiste em forçar o agente a escolher ações aleatoriamente com uma frequência que diminui conforme o treinamento avança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_action(model, state, epsilon=1, n_actions=2):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        return torch.randint(n_actions, (1,)).item()\n",
    "    qvals = model(state)\n",
    "    return torch.argmax(qvals).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede neural \n",
    "\n",
    "A cada passo do treinamento, o agente executará uma ação e utilizará a informação retornada pelo ambiente para calcular uma loss e atualizar seus pesos de forma a minimizá-la. A loss que será utilizada é o erro quadrático médio entre o Q-valor escolhido e o maior Q-valor do próximo estado calculado utilizando a rede com os pesos anteriores à ultima atualização:\n",
    "\n",
    "$$L_i(\\theta_i)=\\mathbb{E}[(y_i - Q(s,a;\\theta_i))^2]$$\n",
    "$$y_i=\\mathbb{E}[R(s')+\\gamma\\max_A Q(s',A;\\theta_{i-1})]$$\n",
    "\n",
    "Note que, portanto, serão necessárias duas redes neurais com a mesma arquitetura, mas uma terá os pesos deefasados em uma iteração com relação à outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_net(model: Qnet, \n",
    "                 model_target: Qnet,\n",
    "                 optimizer: torch.optim.Optimizer, \n",
    "                 batch_of_transitions: List[Transition],\n",
    "                 gamma):\n",
    "    \n",
    "    # convertendo uma lista de Transitions para uma Transition de listas\n",
    "    # mais informacoes: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Transition(*zip(*batch_of_transitions))\n",
    "    \n",
    "    # transformando o batch de estados em um tensor\n",
    "    states = torch.tensor(batch.state)\n",
    "    actions = torch.tensor(batch.action)\n",
    "    rewards = torch.tensor(batch.reward)\n",
    "    next_states = torch.tensor(batch.next_state)\n",
    "    terminated = torch.tensor(batch.terminated)\n",
    "    \n",
    "    predictions = model(states).gather(1, actions.unsqueeze(1)).squeeze() # seleciona o qval da acao tomada\n",
    "    with torch.no_grad():\n",
    "        targets = rewards + gamma * model_target(next_states).max(-1).values * (1 - terminated.int())\n",
    "    \n",
    "    loss = F.mse_loss(predictions, targets)\n",
    "\n",
    "    # atualizando os pesos da rede\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treinamento\n",
    "\n",
    "No loop de treinamento, juntaremos todas as funções desenvolvidas até o momento. A ideia principal é definir um número máximo de episódios (estágio inicial até o estágio final) para que o agente colete experiências do ambiente e otimize sua tabela de QValores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics, show_result=False):\n",
    "    # duration of each episode\n",
    "\n",
    "    # durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    durations = torch.tensor(metrics['episode_durations'], dtype=torch.float)\n",
    "    plt.figure(1)\n",
    "    plt.clf\n",
    "    plt.title('Duration of each episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations)\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations) >= 100:\n",
    "        means = durations.unfold(0, 100, 1).mean(-1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    if not show_result:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_model = None\n",
    "\n",
    "def train(\n",
    "        env: gym.Env, \n",
    "        model: Qnet,\n",
    "        total_timesteps=500000, # numero maximo de episodios\n",
    "        learning_rate=1e-4, # taxa de aprendizado\n",
    "        replay_buffer_size=10_000, # tamanho do replay buffer\n",
    "        gamma=0.99, # fator de desconto\n",
    "        tau = 1.0, # fator de mistura para atualizacao da rede defasada\n",
    "        target_update_freq = 1, # frequencia de atualizacao da rede defasada\n",
    "        batch_size=128, # tamanho do batch\n",
    "        epsilon_0 = 1, # probabilidade inicial de escolher uma ação aleatória\n",
    "        epsilon_f=0.05, # probabilidade final de escolher uma ação aleatória (após o decaimento)\n",
    "        epsilon_decay = 1000, # step in which epsilon will be approximately epsilon_f + 0.36 * (epsilon_0 - epsilon_f)\n",
    "        # learn_starts_in = 10000, # numero de timesteps antes de comecar a treinar\n",
    "        learning_freq = 1, # frequencia de treinamento\n",
    "        verbose=False):\n",
    "\n",
    "    model_target = deepcopy(model)\n",
    "\n",
    "    def eps_scheduler(step):\n",
    "        return epsilon_f + (epsilon_0 - epsilon_f) * math.exp(-1. * step / epsilon_decay)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n",
    "    state, _ = env.reset()\n",
    "    metrics = {\n",
    "        'episode_durations': [],\n",
    "    }\n",
    "    episode_steps = 0\n",
    "\n",
    "    for global_step in range(total_timesteps):\n",
    "        epsilon = eps_scheduler(global_step)\n",
    "\n",
    "        # observe\n",
    "        action = get_action(model, state, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "        episode_steps += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # update\n",
    "        if global_step > batch_size and global_step % learning_freq == 0:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            model = update_q_net(model, model_target, optimizer, batch, gamma)\n",
    "\n",
    "        # update target network\n",
    "        if global_step % target_update_freq == 0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "        # evaluating\n",
    "        if done:\n",
    "            metrics['episode_durations'].append(episode_steps)\n",
    "            episode_steps = 0\n",
    "            print(f'current step: {global_step}, epsilon: {epsilon}, episode: {len(metrics[\"episode_durations\"])}')\n",
    "            state, _ = env.reset()\n",
    "            evaluate(metrics, show_result=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando\n",
    "\n",
    "Está tudo configurado, portanto agora podemos rodar o algoritmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m q_net \u001b[38;5;241m=\u001b[39m Qnet(layer_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m])\n\u001b[1;32m      5\u001b[0m debug_model \u001b[38;5;241m=\u001b[39m deepcopy(q_net)\n\u001b[0;32m----> 6\u001b[0m trained_q_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcartpole\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, model, total_timesteps, learning_rate, replay_buffer_size, gamma, tau, target_update_freq, batch_size, epsilon_0, epsilon_f, epsilon_decay, learning_freq, verbose)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m>\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m learning_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     46\u001b[0m     batch \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 47\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_q_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# update target network\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m, in \u001b[0;36mupdate_q_net\u001b[0;34m(model, model_target, optimizer, batch_of_transitions, gamma)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# atualizando os pesos da rede\u001b[39;00m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "cartpole = gym.make('CartPole-v1')\n",
    "q_net = Qnet(layer_sizes=[128, 128])\n",
    "debug_model = deepcopy(q_net)\n",
    "trained_q_net = train(cartpole, q_net, total_timesteps=15000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o agente\n",
    "\n",
    "A função abaixo rodará um episódio com o agente já treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(total_reward)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_q_net\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(env, q_net, n_episodes, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m     total_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m     25\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_reward\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "def test(env: gym.Env, \n",
    "          q_net,\n",
    "          n_episodes=1,\n",
    "          verbose=False\n",
    "          ):\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(q_net, state, 0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward}\")\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return torch.mean(total_reward)\n",
    "\n",
    "test(gym.make('CartPole-v1', render_mode=\"human\"), trained_q_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
