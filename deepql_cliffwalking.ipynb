{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viniciusandreossi/anaconda3/envs/cleanrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7fd5ad6f8040>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.plots_cliffwalking as plots\n",
    "from env.cliff_walking import WindyCliffWalking\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolvendo o ambiente Cliff Walking com Deep Q-Learning\n",
    "\n",
    "O [Cliff Walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) será utilizado novamente para entendermos o funcionamento do Deep Q-Learning, o algoritmo mais simples de Deep Reinforcement Learning. Recapitulando o objetivo do Cliff Walking, o agente deve ser capaz de atravessar um tabuleiro do início ao fim tomando cuidado para não cair em um penhasco. Se o agente cair no penhasco, ele retorna para o início do tabuleiro e leva uma penalidade de recompensa.\n",
    "\n",
    "<img src=\"media/cliff_walking.gif\" width=\"200\">\n",
    "\n",
    "Abaixo seguem algumas informações importantes para a modelagem do ambiente como um Processo de Decisão de Markov (MDP):\n",
    "\n",
    "### Espaço de ações\n",
    "\n",
    "O espaço de ações é discreto e contém os inteiros do intervalo {0, 3}. Uma ação deve indicar a direção de um movimento:\n",
    "* 0: Cima\n",
    "* 1: Direita\n",
    "* 2: Baixo\n",
    "* 3: Esquerda\n",
    "\n",
    "### Espaço de estados\n",
    "\n",
    "O estado representa a posição do jogador no tabuleiro. Logo, o espaço de estados também é discreto e contém os inteiros do intervalo {0, 47}. O valor numérico da posição do agente no tabuleiro pode ser obtido como linha_atual * nlinhas + coluna, sendo que as linhas e colunas começam em 0.\n",
    "\n",
    "### Recompensas\n",
    "\n",
    "A cada movimento do agente uma penalidade de -1 é aplicada, a menos que o jogador caia do penhasco, o que resulta em uma penalidade -100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo\n",
    "\n",
    "No Deep Q-Learning, os Q-valores de cada ação associados a um estado são calculados através de uma rede neural. Ou seja, a rede neural recebe como entrada um vetor de estados e deve retornar o vetor de Q-valores onde cada elemento representa o Q-valor de uma ação. Um Q-valor pode ser interpretado como \"a recompensa acumulada total esperada por executar a ação A no estado S e depois seguir a mesma política até o final do episódio\".\n",
    "\n",
    "Por se tratar de um problema relativamente simples, o modelo para solucionar o CliffWalking pode ser uma rede MLP. Além disso, note que a predição dos Q-valores é uma tarefa de regressão, portanto não é utilizada uma função de ativação softmax no final da rede. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão de inteiro para (x,y)\n",
    "\n",
    "A rede neural fica mais simples se receber como entrada um par (x,y) em vez de um inteiro. Se fossemos alimentá-la com um inteiro, teríamos que converter esse inteiro para um one-hot. Como são 48 estados, precisaríamos de um vetor de 48 elementos, sendo 47 elementos zerados! Ao passarmos um par (x,y), o vetor fica reduzido para apenas dois elementos. A função abaixo codifica o estado para a posição do agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, layer_sizes: List[int] = [64, 64]):\n",
    "        super().__init__()\n",
    "\n",
    "        # construindo a rede neural\n",
    "        layers = []\n",
    "        input_size = 2 # entrada: posicao (x, y)\n",
    "        for n_neurons in layer_sizes:\n",
    "            layers.append(nn.Linear(input_size, n_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = n_neurons\n",
    "        layers.append(nn.Linear(input_size, 4))\n",
    "        self.nn = nn.Sequential(*layers)\n",
    "\n",
    "    def _encode_state(self, state: int | torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(state, int):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        x = (state % 12) / 11\n",
    "        y = (state // 12) / 3\n",
    "\n",
    "        x = x.reshape(-1, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        return torch.cat([x, y], dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._encode_state(x)\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Replay buffers são utilizados (...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'terminated'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1024, backup_fraction=0.1):\n",
    "        self.backup_size = int(capacity * backup_fraction)\n",
    "        self.queue_size = capacity - self.backup_size\n",
    "        self.backup = []\n",
    "        self.queue = deque([], maxlen=self.queue_size)\n",
    "        self.buffer = []\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\" Save a transition into the buffer.\"\"\"\n",
    "        if len(self.backup) < self.backup_size:\n",
    "            self.backup.append(Transition(*args))\n",
    "        else:\n",
    "            self.queue.append(Transition(*args))\n",
    "        self.buffer = self.backup + list(self.queue)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostrando ações com a política $\\epsilon$-greedy\n",
    "\n",
    "No final do treinamento, espera-se que a melhor ação para cada estado seja aquela cujo Q-Valor é o maior. No entanto, para que o Q-Learning convirja adequadamente, é necessário que no início do treinamento o agente \"explore\" bem o ambiente. Isto é, que o agente visite um grande número de estados mesmo que não sejam necessariamente ótimos. Uma técnica amplamente utilizada para essa finalidade é a política $\\epsilon$-greedy. Ela consiste em forçar o agente a escolher ações aleatoriamente com uma frequência que diminui conforme o treinamento avança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_action(model, state, epsilon, random=True, n_actions=4):\n",
    "    if torch.rand(1) < epsilon and random:\n",
    "        return torch.randint(n_actions, (1,)).item()\n",
    "    qvals = model(state)\n",
    "    return torch.argmax(qvals).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede neural \n",
    "\n",
    "A cada passo do treinamento, o agente executará uma ação e utilizará a informação retornada pelo ambiente para calcular uma loss e atualizar seus pesos de forma a minimizá-la. A loss que será utilizada é o erro quadrático médio entre o Q-valor escolhido e o maior Q-valor do próximo estado calculado utilizando a rede com os pesos anteriores à ultima atualização:\n",
    "\n",
    "$$L_i(\\theta_i)=\\mathbb{E}[(y_i - Q(s,a;\\theta_i))^2]$$\n",
    "$$y_i=\\mathbb{E}[R(s')+\\gamma\\max_A Q(s',A;\\theta_{i-1})]$$\n",
    "\n",
    "Note que, portanto, serão necessárias duas redes neurais com a mesma arquitetura, mas uma terá os pesos deefasados em uma iteração com relação à outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_net(model: Qnet, \n",
    "                 model_target: Qnet,\n",
    "                 optimizer: torch.optim.Optimizer, \n",
    "                 batch_of_transitions: List[Transition],\n",
    "                 gamma):\n",
    "    \n",
    "    # Convert a list of Transitions into a Transition of lists\n",
    "    # Check: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Transition(*zip(*batch_of_transitions))\n",
    "    \n",
    "    # transformando o batch de estados em um tensor\n",
    "    states = torch.tensor(batch.state)\n",
    "    actions = torch.tensor(batch.action)\n",
    "    rewards = torch.tensor(batch.reward)\n",
    "    next_states = torch.tensor(batch.next_state)\n",
    "    terminated = torch.tensor(batch.terminated)\n",
    "    \n",
    "    predictions = model(states).gather(1, actions.unsqueeze(1)).squeeze() # seleciona o qval da acao tomada\n",
    "\n",
    "    # se o estado é terminal, o valor é a recompensa\n",
    "    with torch.no_grad():\n",
    "        targets = rewards + gamma * model_target(next_states).max(-1).values * (1 - terminated.int()) # else rewards + gamma * max_a Q(s', a)\n",
    "    \n",
    "    loss = F.mse_loss(predictions, targets)\n",
    "\n",
    "    # atualizando os pesos da rede\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treinamento\n",
    "\n",
    "No loop de treinamento, juntaremos todas as funções desenvolvidas até o momento. A ideia principal é definir um número máximo de episódios (estágio inicial até o estágio final) para que o agente colete experiências do ambiente e otimize sua tabela de QValores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heatmap:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.heatmap = torch.zeros(rows, cols)\n",
    "    \n",
    "    def update(self, state):\n",
    "        col = state % 12\n",
    "        row = state // 12\n",
    "        self.heatmap[row][col] += 1\n",
    "\n",
    "    def clear(self):\n",
    "        self.heatmap = torch.zeros(self.heatmap.shape)\n",
    "    \n",
    "    def plot(self, episode):\n",
    "        print(self.heatmap.int())\n",
    "        plt.imshow(self.heatmap, cmap='hot', interpolation='nearest')\n",
    "        plt.title(f'Heatmap for the last 20 eps from ep {episode}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics: Dict, show_results=False):\n",
    "    rewards = torch.tensor(metrics['episode_rewards'])\n",
    "    plt.figure(1)\n",
    "    plt.clf\n",
    "    plt.title('Total reward of each episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "    plt.plot(rewards)\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if metrics['avg_reward'] is not None: \n",
    "        x = range(49, 49 + len(metrics['avg_reward']))\n",
    "        plt.plot(x, metrics['avg_reward'].numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    # plot heatmap\n",
    "    metrics['heatmap'].plot(len(metrics['episode_rewards']))\n",
    "\n",
    "    # dump qtable\n",
    "    for i, q in enumerate(metrics['qtable']):\n",
    "        print(f'Q values for state {i}: {q}')\n",
    "\n",
    "    if not show_results:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        env: gym.Env, \n",
    "        model: Qnet,\n",
    "        total_steps=1_000_000,\n",
    "        replay_buffer_size=10_000, # tamanho do replay buffer\n",
    "        batch_size=64, # tamanho do batch\n",
    "        gamma=0.99,\n",
    "        learning_rate=1e-4,\n",
    "        learning_freq=100,\n",
    "        target_update_freq=1000,\n",
    "        tau=1.0,\n",
    "        epsilon_0 = 1, # probabilidade inicial de escolher uma ação aleatória\n",
    "        epsilon_f=0.05, # probabilidade final de escolher uma ação aleatória (após o decaimento)\n",
    "        epsilon_decay = 10_000, # step in which epsilon will be approximately epsilon_f + 0.36 * (epsilon_0 - epsilon_f)\n",
    "        verbose=False):\n",
    "    \n",
    "    model_target = deepcopy(model)\n",
    "\n",
    "    def eps_scheduler(step):\n",
    "        return epsilon_f + (epsilon_0 - epsilon_f) * math.exp(-1. * step / epsilon_decay)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size, backup_fraction=0.0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    metrics = {\n",
    "        'episode_rewards': [],\n",
    "        'avg_reward': None,\n",
    "        'heatmap': Heatmap(4, 12),\n",
    "    }\n",
    "    episode_step = 0\n",
    "    truncated = False\n",
    "\n",
    "    for global_step in range(total_steps):\n",
    "        epsilon = eps_scheduler(global_step)\n",
    "\n",
    "        # observe\n",
    "        action = get_action(model, state, epsilon)\n",
    "        next_state, reward, terminated, _, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "        metrics['heatmap'].update(state)\n",
    "        total_reward += reward\n",
    "        episode_step += 1\n",
    "        if episode_step > 1000:\n",
    "            truncated = True\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # update\n",
    "        if global_step > batch_size and global_step % learning_freq == 0:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            model = update_q_net(model, model_target, optimizer, batch, gamma)\n",
    "\n",
    "        # update target network\n",
    "        if global_step % target_update_freq == 0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "        # evaluating\n",
    "        if done:\n",
    "            metrics['episode_rewards'].append(total_reward)\n",
    "            if len(metrics['episode_rewards']) > 50:\n",
    "                metrics['avg_reward'] = torch.tensor(metrics['episode_rewards']).float().unfold(0, 50, 1).mean(1)\n",
    "                print(f'avg reward: {metrics[\"avg_reward\"][-1]}')\n",
    "            with torch.no_grad():\n",
    "                metrics['qtable'] = model(torch.arange(48))\n",
    "            print(f'current step: {global_step}, epsilon: {epsilon:.2f}')\n",
    "            evaluate(metrics)\n",
    "            total_reward = 0\n",
    "            metrics['heatmap'].clear()\n",
    "            state, _ = env.reset()\n",
    "            episode_step = 0\n",
    "            truncated = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando\n",
    "\n",
    "Está tudo configurado, portanto agora podemos rodar o algoritmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m q_net \u001b[38;5;241m=\u001b[39m Qnet(layer_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m])\n\u001b[1;32m      5\u001b[0m debug_model \u001b[38;5;241m=\u001b[39m deepcopy(q_net)\n\u001b[0;32m----> 6\u001b[0m trained_q_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcliff_walking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, model, total_steps, replay_buffer_size, batch_size, gamma, learning_rate, learning_freq, target_update_freq, tau, epsilon_0, epsilon_f, epsilon_decay, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m action \u001b[38;5;241m=\u001b[39m get_action(model, state, epsilon)\n\u001b[1;32m     39\u001b[0m next_state, reward, terminated, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     42\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheatmap\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(state)\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mReplayBuffer.push\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mappend(Transition(\u001b[38;5;241m*\u001b[39margs))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackup \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "cliff_walking = gym.make('CliffWalking-v0')\n",
    "q_net = Qnet(layer_sizes=[32, 32])\n",
    "debug_model = deepcopy(q_net)\n",
    "trained_q_net = train(cliff_walking, q_net, epsilon_decay=10_000, learning_rate=1e-4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o agente\n",
    "\n",
    "A função abaixo rodará um episódio com o agente já treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env: gym.Env, \n",
    "          q_net,\n",
    "          n_episodes=1,\n",
    "          verbose=False\n",
    "          ):\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(q_net, state, 0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward}\")\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return torch.mean(total_reward)\n",
    "\n",
    "#test(gym.make('CliffWalking-v0', render_mode=\"human\"), trained_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de resultados\n",
    "\n",
    "### Visualizando a política\n",
    "\n",
    "A célula abaixo permitirá observar a ação mais provável de ser tomada em cada uma das posições do tabuleiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24739/1860028285.py:20: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  y = (state // 12) / 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m reload(plots)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_arrows_from_qnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rl_practice_eldorado/utils/plots_cliffwalking.py:64\u001b[0m, in \u001b[0;36mplot_arrows_from_qnet\u001b[0;34m(q_net)\u001b[0m\n\u001b[1;32m     61\u001b[0m normalized_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(qvals\u001b[38;5;241m/\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# cima\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_q_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# direita\u001b[39;00m\n\u001b[1;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39marrow(x, y, normalized_q_values[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m, head_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, head_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, fc\u001b[38;5;241m=\u001b[39ma, ec\u001b[38;5;241m=\u001b[39ma)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/pyplot.py:2347\u001b[0m, in \u001b[0;36marrow\u001b[0;34m(x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39marrow)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marrow\u001b[39m(x, y, dx, dy, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4961\u001b[0m, in \u001b[0;36mAxes.arrow\u001b[0;34m(self, x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   4958\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_xunits(dx)\n\u001b[1;32m   4959\u001b[0m dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(dy)\n\u001b[0;32m-> 4961\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mmpatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFancyArrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_patch(a)\n\u001b[1;32m   4963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view()\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/patches.py:1350\u001b[0m, in \u001b[0;36mFancyArrow.__init__\u001b[0;34m(self, x, y, dx, dy, width, length_includes_head, head_width, head_length, shape, overhang, head_starts_at_zero, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overhang \u001b[38;5;241m=\u001b[39m overhang\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head_starts_at_zero \u001b[38;5;241m=\u001b[39m head_starts_at_zero\n\u001b[0;32m-> 1350\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_verts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverts, closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/patches.py:1409\u001b[0m, in \u001b[0;36mFancyArrow._make_verts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m     length \u001b[38;5;241m=\u001b[39m distance \u001b[38;5;241m+\u001b[39m head_length\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m length:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# display nothing if empty\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;66;03m# start by drawing horizontal arrow, point at (0, 0)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(plots)\n",
    "plots.plot_arrows_from_qnet(debug_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0: [[-0.26136434 -0.26724744 -0.23699465 -0.3733187 ]]\n",
      "state 1: [[-0.27144846 -0.26728043 -0.24119844 -0.37571108]]\n",
      "state 2: [[-0.28178144 -0.26390606 -0.2476211  -0.39044908]]\n",
      "state 3: [[-0.2997111  -0.26615438 -0.26275432 -0.41584584]]\n",
      "state 4: [[-0.32363167 -0.27345696 -0.281278   -0.44857207]]\n",
      "state 5: [[-0.3505416  -0.2850831  -0.30246425 -0.48349568]]\n",
      "state 6: [[-0.37939647 -0.2969428  -0.32455295 -0.518417  ]]\n",
      "state 7: [[-0.4085074  -0.30842635 -0.3466031  -0.5534043 ]]\n",
      "state 8: [[-0.43703473 -0.31941727 -0.36920953 -0.5884064 ]]\n",
      "state 9: [[-0.46534702 -0.3298886  -0.39209569 -0.6232178 ]]\n",
      "state 10: [[-0.49470758 -0.3404794  -0.4149565  -0.6574695 ]]\n",
      "state 11: [[-0.52414346 -0.35103914 -0.43799382 -0.6916231 ]]\n",
      "state 12: [[-0.38011822 -0.4195511  -0.38666034 -0.5301238 ]]\n",
      "state 13: [[-0.39039123 -0.4392437  -0.38741583 -0.53646696]]\n",
      "state 14: [[-0.40114212 -0.46237087 -0.38874257 -0.5412873 ]]\n",
      "state 15: [[-0.4145421 -0.4800888 -0.3915658 -0.5552423]]\n",
      "state 16: [[-0.43216017 -0.49295446 -0.3985632  -0.58459055]]\n",
      "state 17: [[-0.45440716 -0.49816725 -0.4106456  -0.61639047]]\n",
      "state 18: [[-0.47673863 -0.5043531  -0.42619157 -0.64756334]]\n",
      "state 19: [[-0.4995195  -0.51149255 -0.44233668 -0.6799813 ]]\n",
      "state 20: [[-0.52251875 -0.5189599  -0.4583013  -0.71260774]]\n",
      "state 21: [[-0.5455179  -0.52642715 -0.4742658  -0.74523413]]\n",
      "state 22: [[-0.56988907 -0.53333205 -0.49040818 -0.77837914]]\n",
      "state 23: [[-0.5941496  -0.54019344 -0.5066696  -0.81157273]]\n",
      "state 24: [[-0.5619465  -0.57912993 -0.56196946 -0.73784244]]\n",
      "state 25: [[-0.56248903 -0.5895686  -0.5698579  -0.74041826]]\n",
      "state 26: [[-0.5683794  -0.60439146 -0.57498276 -0.74948347]]\n",
      "state 27: [[-0.5818608  -0.62278503 -0.58164644 -0.7578457 ]]\n",
      "state 28: [[-0.5965786  -0.64745224 -0.58339363 -0.7684652 ]]\n",
      "state 29: [[-0.610525  -0.6687128 -0.585553  -0.7867645]]\n",
      "state 30: [[-0.62330973 -0.68331325 -0.59060913 -0.8077197 ]]\n",
      "state 31: [[-0.6381496 -0.6985185 -0.5968288 -0.8311638]]\n",
      "state 32: [[-0.6556748 -0.7120305 -0.6021731 -0.856969 ]]\n",
      "state 33: [[-0.6784215  -0.7242943  -0.6096917  -0.88439476]]\n",
      "state 34: [[-0.7022481 -0.7338002 -0.6210206 -0.9131975]]\n",
      "state 35: [[-0.7261969 -0.7417526 -0.6353321 -0.9442295]]\n",
      "state 36: [[-0.74628234 -0.73831666 -0.7379662  -0.94944257]]\n",
      "state 37: [[-0.7421775 -0.745317  -0.7499856 -0.9523979]]\n",
      "state 38: [[-0.7424053  -0.75632924 -0.7577844  -0.95572776]]\n",
      "state 39: [[-0.75013304 -0.76710457 -0.76619804 -0.9647643 ]]\n",
      "state 40: [[-0.76339686 -0.7850983  -0.77310526 -0.97323734]]\n",
      "state 41: [[-0.78013074 -0.8126976  -0.7758324  -0.9906554 ]]\n",
      "state 42: [[-0.79611135 -0.8409838  -0.77691275 -1.0031328 ]]\n",
      "state 43: [[-0.80980444 -0.85726774 -0.78217745 -1.0205238 ]]\n",
      "state 44: [[-0.8230245  -0.87236845 -0.78602064 -1.041196  ]]\n",
      "state 45: [[-0.83635676 -0.8873336  -0.78890204 -1.0605172 ]]\n",
      "state 46: [[-0.8505796 -0.9021256 -0.7924093 -1.0801271]]\n",
      "state 47: [[-0.8671372  -0.91789716 -0.79809195 -1.1032377 ]]\n"
     ]
    }
   ],
   "source": [
    "for state in range(48):\n",
    "    print(f'state {state}: {debug_model(state).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando a influência da taxa de aprendizado\n",
    "\n",
    "A célula abaixo criará um gráfico com a recompensa total média de um conjunto de agentes treinados com uma diferente taxa de aprendizado. Como você explicaria os resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[1;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mplot_avg_return_x_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mplot_avg_return_x_alpha\u001b[0;34m(n_samples, n_alphas)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_avg_return_x_alpha\u001b[39m(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m ):\n\u001b[1;32m      4\u001b[0m     avg_returns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m     alphas\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_alphas)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[1;32m      8\u001b[0m         q_table \u001b[38;5;241m=\u001b[39m new_q_table(n_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, n_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# retorno médio esperado em função de alpha\n",
    "\n",
    "def plot_avg_return_x_alpha(n_samples=100, n_alphas=10 ):\n",
    "    avg_returns = []\n",
    "    alphas=np.linspace(0.1, 1, n_alphas)\n",
    " \n",
    "    for alpha in alphas:\n",
    "        q_table = new_q_table(n_states=48, n_actions=4)\n",
    "        trained_q_table = train(cliff_walking, q_table, alpha=alpha)\n",
    "        avg_returns.append(test(cliff_walking, trained_q_table, n_samples))\n",
    "\n",
    "    plt.plot(alphas, avg_returns)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_avg_return_x_alpha(n_samples=100, n_alphas=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício - ambientes não-determinísticos\n",
    "\n",
    "Agora que já vimos que o agente aprendeu um caminho para o objetivo que minimiza a distância caminhada e não cai no penhasco, está na hora de deixar as coisas um pouco mais difíceis. Utilizaremos agora uma versão modificada do Cliff Walking que adiciona vento aleatório na direção do penhasco. Ou seja, existe uma probabilidade do agente ser deslocado uma casa para baixo de maneira involuntária.\n",
    "\n",
    "Essa alteração será interessante para observarmos a infuência da taxa de aprendizado no treinamento de um modelo robusto a ambientes não-determinísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windy_cliff_walking = WindyCliffWalking(wind=0.5)\n",
    "\n",
    "windy_q_table = new_q_table(n_states=48, n_actions=4)\n",
    "trained_windy_q_table = train(cliff_walking, windy_q_table, alpha=0.75, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windy_cliff_walking.render_mode = 'human'\n",
    "test(windy_cliff_walking, trained_windy_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_arrows(trained_q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
