{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from modules.loggers import TabQLogger\n",
    "from modules.algorithm import AbstractAlgorithm\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "no_render_env = gym.make(\n",
    "    \"CliffWalking-v0\",\n",
    ")\n",
    "\n",
    "render_env = gym.make(\n",
    "    \"CliffWalking-v0\",\n",
    "    render_mode = 'human'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQLearn(AbstractAlgorithm):\n",
    "    def __init__(self, \n",
    "                 env: gym.Env,\n",
    "                 *args,\n",
    "                 alpha: float = 0.1, \n",
    "                 gamma: float = 0.99, \n",
    "                 epsilon0: float = 0.9,\n",
    "                 epsilon_decay: float = 0.999,\n",
    "                 q_init: float = 0.6, \n",
    "                 **kwargs):\n",
    "        super().__init__(env, *args, **kwargs)\n",
    "        self.logger = TabQLogger()\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon0\n",
    "        self._epsilon_decay = epsilon_decay\n",
    "        self._q_init = q_init\n",
    "        \n",
    "        self._q_table = defaultdict(lambda: np.full(self._n_actions, self._q_init))\n",
    "\n",
    "        self.hyperparams = {\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'q_init': q_init,\n",
    "            'epsilon0': epsilon0,\n",
    "            'epsilon_decay': epsilon_decay\n",
    "        }\n",
    "\n",
    "    def _encode_observation(self, observation):\n",
    "        if self.is_discrete:\n",
    "            return str(observation)\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        #return np.minimum(self.n // 2, np.maximum(-self.n // 2, (observation / self.state_step_size).astype('int64'))).__str__()\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        \"\"\"Follow e-greedy policy to get action for a given state.\"\"\"\n",
    "        qvals = self._q_table[self._encode_observation(observation)]\n",
    "\n",
    "        # e-greedy policy\n",
    "        if np.random.rand() < self._epsilon:\n",
    "            return np.random.choice(self._n_actions)\n",
    "        \n",
    "        return np.argmax(qvals)\n",
    "\n",
    "    def _update(self, observation, action, reward, next_observation):\n",
    "        \"\"\"\n",
    "        Updates the Q values using Bellman's equation.\n",
    "        \"\"\"\n",
    "        observation = self._encode_observation(observation)\n",
    "        next_observation = self._encode_observation(next_observation)\n",
    "\n",
    "        # Bellman's equation\n",
    "        self._q_table[observation][action] = (1 - self._alpha) * self._q_table[observation][action] + self._alpha * (\n",
    "            reward + self._gamma * np.max(self._q_table[next_observation]))\n",
    "        \n",
    "    def train(self, n_episodes):\n",
    "        CHECKPOINT_INTERVAL = 1000\n",
    "        self.logger.write_hyperparameters(self.hyperparams)\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            observation, info = self._env.reset()\n",
    "            total_reward = 0\n",
    "            win = False\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_action(observation)\n",
    "                next_observation, reward, terminated, truncated, info = self._env.step(action)\n",
    "                self._update(observation, action, reward, next_observation)\n",
    "                observation = next_observation\n",
    "                total_reward += reward\n",
    "\n",
    "                done = terminated or truncated\n",
    "                if terminated:\n",
    "                    self._set_terminal_observation(observation)\n",
    "\n",
    "            self.logger.log('total_reward', total_reward)\n",
    "            #self.logger.log('win', win)\n",
    "\n",
    "            if episode % CHECKPOINT_INTERVAL == 0 and episode > 0: \n",
    "                self.logger.save_checkpoint(self._q_table, f'qvalues.pkl')\n",
    "                print(f'Episode {episode}/{n_episodes} | epsilon: {self._epsilon} | mean total reward: {np.mean(self.logger.metrics[\"total_reward\"][-50:])}')\n",
    "            \n",
    "            if self._epsilon > 0.001:\n",
    "                self._epsilon *= self._epsilon_decay\n",
    "        \n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self._q_table = pickle.load(f)    \n",
    "\n",
    "    def _set_terminal_observation(self, terminal_observation):\n",
    "        # sets all the QValues for a terminal observation to zero \n",
    "        self._q_table[self._encode_observation(terminal_observation)] = np.zeros(self._n_actions)\n",
    "    \n",
    "    def render(self, env: gym.Env, n_episodes: int = 1, state_dict_path: str = None):\n",
    "        assert env.render_mode == 'human', 'Render mode must be set to human'\n",
    "\n",
    "        if os.path.exists(state_dict_path):\n",
    "            self.load(state_dict_path)\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            observation, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_action(observation)\n",
    "                observation, _, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/10000 | epsilon: 0.33092588229386716 | mean total reward: -148.48\n",
      "Episode 2000/10000 | epsilon: 0.12167993285774943 | mean total reward: -56.82\n",
      "Episode 3000/10000 | epsilon: 0.04474115459823254 | mean total reward: -28.32\n",
      "Episode 4000/10000 | epsilon: 0.0164511178447405 | mean total reward: -19.52\n",
      "Episode 5000/10000 | epsilon: 0.006049000763879044 | mean total reward: -15.16\n",
      "Episode 6000/10000 | epsilon: 0.0022241899053143913 | mean total reward: -13.04\n",
      "Episode 7000/10000 | epsilon: 0.0009999929953143968 | mean total reward: -13.04\n",
      "Episode 8000/10000 | epsilon: 0.0009999929953143968 | mean total reward: -13.0\n",
      "- Current mean reward: -13.02, but best mean reward is: -13.0\n",
      "- Mean reward did not improve. Checkpoint not saved.\n",
      "Episode 9000/10000 | epsilon: 0.0009999929953143968 | mean total reward: -13.02\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "tabq = TabularQLearn(env)\n",
    "tabq.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabq = TabularQLearn(gym.make(\"CliffWalking-v0\", render_mode = 'human'))\n",
    "tabq.render(gym.make(\"CliffWalking-v0\", render_mode = 'human'), 10, 'results/qvalues.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
