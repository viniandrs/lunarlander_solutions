{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolvendo o ambiente Lunar Lander com Deep Q-Learning\n",
    "\n",
    "O [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) possui um espaço de estados contínuo o que torna muito mais difícil a sua solução por métodos tabulares. Sendo assim, esse ambiente é ideal para a aplicação do Deep Q-Learning e suas variações. O objetivo principal do Lunar Lander é usar três motorers de uma nave espacial para pousá-la o mais perto possível de uma plataforma de pouso. Um episódio é definido como vitória/derrota com base na pontuação final obtida.\n",
    "\n",
    "<img src=\"media/lunar_lander.gif\" width=\"200\">\n",
    "\n",
    "Abaixo seguem algumas informações importantes para a modelagem do ambiente como um Processo de Decisão de Markov (MDP):\n",
    "\n",
    "### Espaço de ações\n",
    "\n",
    "O espaço de ações é discreto e contém os inteiros do intervalo {0, 3}. Uma ação deve indicar a direção de um movimento:\n",
    "* 0: Não fazer nada\n",
    "* 1: Ativar motor esquerdo\n",
    "* 2: Ativar motor principal\n",
    "* 3: Ativar motor direito\n",
    "\n",
    "### Espaço de estados\n",
    "\n",
    "O vetor de estado possui oito elementos: coordenadas x & y, velocidades x & y, ângulo, velocidade de rotação e duas variáveis booleanas que indicam se as pernas da nave estão ou não tocando o chão. \n",
    "\n",
    "### Recompensas\n",
    "\n",
    "A cada passo temporal, a recompensa:\n",
    "\n",
    "* é aumentada/diminuída se a nave se aproximar/distanciar da plataforma de pouso.\n",
    "* é aumentada/diminuída se a nave se movimentar mais devagar/rápido.\n",
    "* é diminuída quanto mais a nave estiver inclinada.\n",
    "* é aumentada em 10 pontos para cada perna da nave que estiver tocando o chão.\n",
    "* é diminuída em 0.03 pontos para cada frame em que um motor lateral estiver ativo.\n",
    "* é diminuída em 0.3 pontos para cada frame em que o motor principal estiver ativo.\n",
    "\n",
    "No final do episódio, uma recompensa adicional de -100 ou +100 é atribuída caso a a nave colida ou pouse com segurança.\n",
    "\n",
    "Caso a pontuação final seja maior que 200, o episódio é considerado como solução.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo\n",
    "\n",
    "No Deep Q-Learning, os Q-valores de cada ação associados a um estado são calculados através de uma rede neural. Ou seja, a rede neural recebe como entrada um vetor de estados e deve retornar o vetor de Q-valores onde cada elemento representa o Q-valor de uma ação. Um Q-valor pode ser interpretado como \"a recompensa acumulada total esperada por executar a ação A no estado S e depois seguir a mesma política até o final do episódio\".\n",
    "\n",
    "O vetor de estados do Lunar Lander já vem em um formato adequado para a entrada de uma rede neural então não é necvessário formatá-lo, apenas transformá-lo em um tensor do PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, layer_sizes: List[int] = [64, 64], n_actions=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # construindo a rede neural\n",
    "        layers = []\n",
    "        input_size = 8\n",
    "        for n_neurons in layer_sizes:\n",
    "            layers.append(nn.Linear(input_size, n_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = n_neurons\n",
    "        layers.append(nn.Linear(input_size, n_actions))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "        self.nn = nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "        self.rewards = []\n",
    "        self.logprobs = []\n",
    "\n",
    "    def _encode_state(self, state) -> torch.Tensor:\n",
    "        return torch.tensor(state, dtype=torch.float32).float().to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._encode_state(x)\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Replay buffers são utilizados para armazenar transições de estado observadas durante o processo de treinamento. A rede neural utiliza batches de transições (não necessariamente sequenciais!) para calcular a loss e atualizar seus pesos. O uso de replay buffers torna o processo de aprendizado mais eficiente pois permite que a rede aprenda a partir de uma transição várias vezes além de ajudar a aumentar a estabilidade do treinamento da rede neural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1024, backup_fraction=0.1):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\" Save a transition into the buffer.\"\"\"\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostrando ações com a política $\\epsilon$-greedy\n",
    "\n",
    "No final do treinamento, espera-se que a melhor ação para cada estado seja aquela cujo Q-Valor é o maior. No entanto, para que o Q-Learning convirja adequadamente, é necessário que no início do treinamento o agente \"explore\" bem o ambiente. Isto é, que o agente visite um grande número de estados mesmo que não sejam necessariamente ótimos. Uma técnica amplamente utilizada para essa finalidade é a política $\\epsilon$-greedy. Ela consiste em forçar o agente a escolher ações aleatoriamente com uma frequência que diminui conforme o treinamento avança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy: PolicyNet, state):\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.logprobs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da rede neural \n",
    "\n",
    "A cada passo do treinamento, o agente executará uma ação e utilizará a informação retornada pelo ambiente para calcular uma loss e atualizar seus pesos de forma a minimizá-la. A loss que será utilizada é o erro quadrático médio entre o Q-valor escolhido e o maior Q-valor do próximo estado calculado utilizando a rede com os pesos anteriores à ultima atualização:\n",
    "\n",
    "$$L_i(\\theta_i)=\\mathbb{E}[(y_i - Q(s,a;\\theta_i))^2]$$\n",
    "$$y_i=\\mathbb{E}[R(s')+\\gamma\\max_A Q(s',A;\\theta_{i-1})]$$\n",
    "\n",
    "Note que, portanto, serão necessárias duas redes neurais com a mesma arquitetura, mas uma terá os pesos deefasados em uma iteração com relação à outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy_net(\n",
    "        policy: PolicyNet, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        gamma,\n",
    "        normalize_rewards=False):\n",
    "    \n",
    "    if len(policy.rewards) != len(policy.logprobs):\n",
    "            raise ValueError(\"Trajectory and probability buffer have different lengths\")\n",
    "        \n",
    "    # calculating and normalizing the discounted total reward for each step\n",
    "    with torch.no_grad():\n",
    "        rewards = torch.tensor([reward for reward in policy.rewards], device=DEVICE)\n",
    "        discounts = gamma ** torch.arange(len(rewards), dtype=torch.float64, device=DEVICE)\n",
    "        discounted_rewards = rewards * discounts\n",
    "        cumsum_rewards = discounted_rewards - discounted_rewards.cumsum(dim=-1) + discounted_rewards.sum(dim=-1)\n",
    "        returns = cumsum_rewards / discounts\n",
    "\n",
    "        if normalize_rewards:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "    # policy gradient update\n",
    "    policy_loss = (returns * discounts * torch.cat(policy.logprobs)).sum(dim=-1) # hstack -> cat\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de treinamento\n",
    "\n",
    "No loop de treinamento, juntaremos todas as funções desenvolvidas até o momento. A ideia principal é definir um número máximo de episódios (estágio inicial até o estágio final) para que o agente colete experiências do ambiente e otimize sua tabela de QValores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(metrics: \u001b[43mDict\u001b[49m, show_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_rewards\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(metrics: Dict, show_result=False):\n",
    "    rewards = torch.tensor(metrics['episode_rewards'])\n",
    "    plt.figure(1)\n",
    "    plt.clf\n",
    "    plt.title('Total reward of each episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "    plt.grid()\n",
    "    plt.plot(rewards)\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if metrics['avg_reward'] is not None: \n",
    "        x = range(49, 49 + len(metrics['avg_reward']))\n",
    "        plt.plot(x, metrics['avg_reward'].numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    if not show_result:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m         env: \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mEnv, \n\u001b[1;32m      3\u001b[0m         model: Qnet,\n\u001b[1;32m      4\u001b[0m         total_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m450_000\u001b[39m,\n\u001b[1;32m      5\u001b[0m         replay_buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m, \u001b[38;5;66;03m# tamanho do replay buffer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, \u001b[38;5;66;03m# tamanho do batch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m      8\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m,\n\u001b[1;32m      9\u001b[0m         learning_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m         target_update_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     11\u001b[0m         tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     12\u001b[0m         epsilon_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m# probabilidade inicial de escolher uma ação aleatória\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         epsilon_f\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;66;03m# probabilidade final de escolher uma ação aleatória (após o decaimento)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         epsilon_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000\u001b[39m, \u001b[38;5;66;03m# step in which epsilon will be approximately epsilon_f + 0.36 * (epsilon_0 - epsilon_f)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     17\u001b[0m     model_target \u001b[38;5;241m=\u001b[39m deepcopy(model)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meps_scheduler\u001b[39m(step):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "        env: gym.Env, \n",
    "        policy: PolicyNet,\n",
    "        n_episodes=1000,\n",
    "        ep_max_steps=1000,\n",
    "        gamma=0.99,\n",
    "        learning_rate=5e-4,\n",
    "        verbose=False):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "    \n",
    "    metrics = {\n",
    "        'episode_rewards': [],\n",
    "        'avg_reward': None,\n",
    "    }\n",
    "    episode_step = 0\n",
    "    truncated = False\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # observe\n",
    "        for step in range(ep_max_steps):\n",
    "            action = get_action(policy, state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "\n",
    "            # logging\n",
    "            if terminated or truncated:\n",
    "                if not verbose:\n",
    "                    break\n",
    "                \n",
    "                metrics['episode_rewards'].append(ep_reward)\n",
    "                if len(metrics['episode_rewards']) > 50:\n",
    "                    metrics['avg_reward'] = torch.tensor(metrics['episode_rewards']).float().unfold(0, 50, 1).mean(1)\n",
    "                    print(f'avg reward: {metrics[\"avg_reward\"][-1]}')\n",
    "                evaluate(metrics)\n",
    "                break\n",
    "\n",
    "        policy = update_policy_net(policy, optimizer, gamma)\n",
    "\n",
    "    evaluate(metrics, show_result=True)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando\n",
    "\n",
    "Está tudo configurado, portanto agora podemos rodar o algoritmo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lunar_lander \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m q_net \u001b[38;5;241m=\u001b[39m Qnet()\n\u001b[1;32m      3\u001b[0m trained_q_net \u001b[38;5;241m=\u001b[39m train(lunar_lander, q_net, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "cart_pole = gym.make('CartPole-v2')\n",
    "policy_net = PolicyNet()\n",
    "trained_policy_net = train(cart_pole, policy_net, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o agente\n",
    "\n",
    "A função abaixo rodará um episódio com o agente já treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(261.2986)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(env: gym.Env, \n",
    "          q_net,\n",
    "          n_episodes=1,\n",
    "          verbose=False\n",
    "          ):\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(q_net, state, 0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward}\")\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    total_reward = torch.tensor(total_rewards, dtype=torch.float32)\n",
    "    return torch.mean(total_reward)\n",
    "\n",
    "test(gym.make('LunarLander-v2', render_mode=\"human\"), trained_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício - ambientes não-determinísticos\n",
    "\n",
    "O Lunar Lander, assim como o Cliff Walking, também possui uma versão com vento aleatório que faz com que o ambiente se torne não determinístico. Tente encontrar uma taxa de aprendizado que permita solucionar essa versão do ambiente! Ela é maior ou menor que a taxa utilizada anteriormente? O efeito de um ambiente não determinístico sobre a taxa de aprendizado é análogo ao observado no algoritmo tabular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nd_lunar_lander \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, enable_wind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m nd_q_net \u001b[38;5;241m=\u001b[39m Qnet()\n\u001b[0;32m----> 3\u001b[0m trained_nd_q_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnd_lunar_lander\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnd_q_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m450_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, model, total_steps, replay_buffer_size, batch_size, gamma, learning_rate, learning_freq, target_update_freq, tau, epsilon_0, epsilon_f, epsilon_decay, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# observe\u001b[39;00m\n\u001b[1;32m     37\u001b[0m action \u001b[38;5;241m=\u001b[39m get_action(model, state, epsilon)\n\u001b[0;32m---> 38\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state, terminated)\n\u001b[1;32m     40\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:787\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nd_lunar_lander = gym.make('LunarLander-v2', enable_wind=True, render_mode=\"human\")\n",
    "nd_q_net = Qnet()\n",
    "trained_nd_q_net = train(nd_lunar_lander, nd_q_net, total_steps=450_000, epsilon_decay=100_000, learning_rate=5e-4, replay_buffer_size=2 ** 20, batch_size=128, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
